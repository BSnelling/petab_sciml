var documenterSearchIndex = {"docs":
[{"location":"net_activation.html#layers_activation","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The PEtab SciML neural network model YAML format supports numerous standard neural network layers and activation functions. Layer names and associated keyword arguments follow the PyTorch naming scheme. PyTorch is used because it is currently the most popular machine learning framework, and its comprehensive documentation makes it easy to look up details for any specific layer or activation function.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"If support is lacking for a layer or activation function you would like to see, please file an issue on GitHub.","category":"page"},{"location":"net_activation.html#Supported-Neural-Network-Layers","page":"Supported Layers and Activation Functions","title":"Supported Neural Network Layers","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The table below lists the supported and tested neural network layers along with links to their respective PyTorch documentation. Additionally, the table indicates which tools support each layer.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"layer PEtab.jl AMICI\nLinear ✔️ \nBilinear ✔️ \nFlatten ✔️ \nDropout ✔️ \nDropout1d ✔️ \nDropout2d ✔️ \nDropout3d ✔️ \nAlphaDropout ✔️ \nConv1d ✔️ \nConv2d ✔️ \nConv3d ✔️ \nConvTranspose1d ✔️ \nConvTranspose2d ✔️ \nConvTranspose3d ✔️ \nMaxPool1d ✔️ \nMaxPool2d ✔️ \nMaxPool3d ✔️ \nAvgPool1d ✔️ \nAvgPool2d ✔️ \nAvgPool3d ✔️ \nLPPool1 ✔️ \nLPPool2 ✔️ \nLPPool3 ✔️ \nAdaptiveMaxPool1d ✔️ \nAdaptiveMaxPool2d ✔️ \nAdaptiveMaxPool3d ✔️ \nAdaptiveAvgPool1d ✔️ \nAdaptiveAvgPool2d ✔️ \nAdaptiveAvgPool3d ✔️ ","category":"page"},{"location":"net_activation.html#Supported-Activation-Function","page":"Supported Layers and Activation Functions","title":"Supported Activation Function","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The table below lists the supported and tested activation functions along with links to their respective PyTorch documentation. Additionally, the table indicates which tools support each layer.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"Function PEtab.jl AMICI\nrelu ✔️ \nrelu6 ✔️ \nhardtanh ✔️ \nhardswish ✔️ \nselu ✔️ \nleaky_relu ✔️ \ngelu ✔️ \ntanhshrink ✔️ \nsoftsign ✔️ \nsoftplus ✔️ \ntanh ✔️ \nsigmoid ✔️ \nhardsigmoid ✔️ \nmish ✔️ \nelu ✔️ \ncelu ✔️ \nsoftmax ✔️ \nlog_softmax ✔️ ","category":"page"},{"location":"test_info.html#Test-Suite","page":"Test Suite","title":"Test Suite","text":"","category":"section"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"The PEtab SciML format provides an extensive test suite to verify the correctness of tools that support the standard. The tests are divided into three parts, which are recomended to be run in this order:","category":"page"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"Neural network import\nInitialization (start guesses for parameter estimation)\nHybrid models that combine mechanistic and data-driven components","category":"page"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"The neural network tests cover a relatively large set of architectures. The hybrid tests, by comparison, involve fewer network architectures, because if the hybrid interface works with a given library (e.g., Equinox or Lux.jl) and the implementation cleanly separates neural network and dynamic components, the combination should function correctly once network models can be imported properly.","category":"page"},{"location":"test_info.html#Neural-network-import-tests","page":"Test Suite","title":"Neural-network import tests","text":"","category":"section"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"The neural networks test different layers and activation functions. A complete list of tested layers and activation functions can be found here.","category":"page"},{"location":"test_info.html#Initialization-tests","page":"Test Suite","title":"Initialization tests","text":"","category":"section"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"These tests ensure that nominal parameter values are read correctly and that initializationPriorType is properly implemented. For the test cases, either the nominal values are directly verified, or, when testing start guesses that are randomly sampled, the mean and variance of multiple samples are evaluated.","category":"page"},{"location":"test_info.html#Hybrid-Models-Test","page":"Test Suite","title":"Hybrid Models Test","text":"","category":"section"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"For each case, the following aspects are tested:","category":"page"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"Correct evaluation of the model likelihood.\nAccuracy of simulated values when solving the model forward in time.\nGradient correctness. This is particularly important for SciML models, where computing gradients can be challenging.","category":"page"},{"location":"test_info.html","page":"Test Suite","title":"Test Suite","text":"If a tool supports providing the neural network model in a format other than the YAML fromat, we recommend modifying the tests in this folder to use another neural network format to verify the correctness of the implementation.","category":"page"},{"location":"format.html#Format-Specification","page":"Format","title":"Format Specification","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"A PEtab SciML problem extends the PEtab standard version 2 to accommodate hybrid models (SciML problems) that combine neural network (neural net) and mechanistic components. The extension introduces two new PEtab file types:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Neural Net File(s): Optional YAML file(s) describing neural net model(s).\nHybridization table: Table used to assign neural network outputs and inputs.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It further extends the following standard PEtab files:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Mapping Table: Used to describe how neural network inputs and outputs map to PEtab variables.\nParameters Table: Used to describe nominal values and potential priors for initializing network parameters.\nProblem YAML File: Includes a new SciML field for neural network models and (optionally) array or tensor formatted data.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"All other PEtab files remain unchanged. This page explains the format and options for each file that is added or modified by the PEtab SciML extension.","category":"page"},{"location":"format.html#High-Level-Overview","page":"Format","title":"High Level Overview","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The aim of the PEtab SciML extension is to facilitate the creation of hybrid models that combine neural network and mechanistic components. The extension is designed to keep the dynamic model, neural network model, and PEtab problem as independent as possible, with the models linked in the hybridization and/or condition tables.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"The extension supports three hybrid model types, and a valid PEtab SciML problem can use one or any combination of these types. Each of the three types is specified differently:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Neural network models in the ODE model’s right-hand side (RHS): In this scenario, the model (e.g. SBML) file is modified during import by either replacing a derivative or setting a parameter value to a neural network output.\nNeural network models in the observable function: In this scenario, the neural network output variable (as defined in the mapping table) is directly inserted in the observable formula.\nNeural network models to parametrize ODEs: In this scenario, the neural network model sets constant parameters or initial values in the ODE model prior to simulation.","category":"page"},{"location":"format.html#net_format","page":"Format","title":"Neural Network Model Format","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The neural network model format is flexible, meaning that neural network models can be provided in any format supported by tools compatible with the PEtab SciML format (for example, Lux.jl in PEtab.jl). Additionally, the petab_sciml library offers a YAML file format (see below) for neural network models, which can be imported into tools across programming languages. The reason for this flexibility in format is that, although the YAML format can accommodate many architectures, some may still be difficult to represent. Still, when possible, we recommend using the YAML format to facilitate model exchange across different software.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Regardless of the model format, to be compatible with the other files in a PEtab SciML problem a neural network model must include two main parts:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"layers: A constructor that defines the network layers, each with a unique identifier.\nforward: A forward pass function that, given input arguments, specifies the order in which layers are called, applies any activation functions, and returns a single array as output. The forward function can accept more than one input argument (n > 1), and in the mapping table, the forward function's nth input argument (ignoring any potential class arguments such as self) is considered as argument n.","category":"page"},{"location":"format.html#YAML_net_format","page":"Format","title":"YAML Network file format","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The petab_sciml library provides a YAML file format for neural network model exchange, where layer names and argument syntax follow PyTorch conventions. Although the YAML files can be written manually, the recommended approach is to define a PyTorch nn.Module—using the constructor to set up the layers and the forward method to specify how they are invoked. For example, a simple feed-forward network can be defined as:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Linear(in_features=2, out_features=5)\n        self.layer2 = nn.Linear(in_features=5, out_features=5)\n        self.layer3 = nn.Linear(in_features=5, out_features=1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = F.tanh(x)\n        x = self.layer2(x)\n        x = F.tanh(x)\n        x = self.layer3(x)\n        return x","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"The corresponding YAML file can then be generated using the petab_sciml library:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"# TODO: Add when syntax is decided upon","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Any PyTorch-supported keyword can be supplied for each layer in the YAML file, allowing for a broad range of architectures. For example, a more complex convolutional model could be created by:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.max_pool1 = nn.MaxPool2d((2, 2))\n        self.fc1 = nn.Linear(64, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.fc4 = nn.Linear(10, 10)\n        self.flatten1 = nn.Flatten()\n\n    def forward(self, input1, input2):\n        # input1\n        c1 = self.conv1(input1)\n        s2 = self.max_pool1(c1)\n        c3 = self.conv2(s2)\n        s4 = self.max_pool1(c3)\n        s4 = self.flatten1(s4)\n        f5 = self.fc1(s4)\n        f6 = self.fc2(f5)\n        f8 = self.fc3(f6)\n        # input 2\n        f9 = self.fc4(input2)\n        output = f8 + f9\n        return output","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Where in this case, the forward function has two inputs arguments. A complete list of supported and tested layers and activation functions can be found here.","category":"page"},{"location":"format.html#hdf5_ps_structure","page":"Format","title":"Neural Network Parameters","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"All parameters for a neural network model are stored in an HDF5 file, with the file path specified in the problem YAML file. In this file, each layer’s parameters are in a group with identifier f.layerId, where layerId is the layer’s unique identifier. More formally, an HDF5 parameter file should have the following structure for an arbitrary number of layers:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"parameters.hdf5\n└───layer1 (group)\n│   ├── arrayId{1}\n│   └── arrayId{2}\n└───layer2 (group)\n    ├── arrayId{1}\n    └── arrayId{2}","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Here, arrayId depends on the naming convention for parameters in each layer. For example, a PyTorch linear layer typically has arrays named weight and (optionally) bias. While these names are common in many layers, the actual arrayId depends on the layer type and the specific neural network library.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Because parameters are stored in HDF5, they are saved as arrays. The indexing convention and naming therefore depends on the model library:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"For neural network models in the PEtab SciML YAML format, indexing follows PyTorch conventions. Usually, users do not need to handle these details directly, as PEtab SciML tools manage them automatically. This also means that arrayId follows PyTorch naming convention.\nFor neural networks provided by another library, the indexing and arrayId naming follow that library’s conventions.","category":"page"},{"location":"format.html#hdf5_input_structure","page":"Format","title":"Neural Network Input","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"When network input is provided as an array via an HDF5 file (see the mapping table below), its format should be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"input.hdf5\n└───input (group)\n    └─── input_array","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"As with parameters, the indexing depends on the neural network library:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"For neural network models in the PEtab SciML format, indexing follows PyTorch conventions. For example, if the first layer is Conv2d, the input should be in (C, W, H) format, with data stored in row-major order. In general, the input should be structured to be directly compatible with PyTorch.\nFor neural networks provided by another library, the indexing and ordering follow the conventions of that library.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"tip: For developers: Respect memory order\nTools supporting the SciML extension should, for computational efficiency, reorder input data and potential layer parameter arrays to match the memory ordering of the target language. For example, PEtab.jl converts input data to column-major order, with Julia indexing.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"TODO: We will fix condition specific input in the YAML file later.","category":"page"},{"location":"format.html#mapping_table","page":"Format","title":"Mapping Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"To avoid confusion regarding what a neural network ID (netId) refers to (e.g., parameters, inputs, etc.), netId is not considered a valid PEtab identifier. Consequently, every neural network input, parameter, and output must be explicitly mapped in the mapping table to a PEtab variable. In the context of the PEtab SciML extension, the relevant mapping table columns are:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId [STRING]: A valid PEtab identifier that is not defined elsewhere in the PEtab problem. This identifier can be referenced in the condition, hybridization, measurement, parameter, and observable tables, but not within the model itself.\nmodelEntityId [STRING]: Describes the neural network entity corresponding to the petabEntityId. For a neural network, the valid identifiers are:  \nnetId.parameters: Parameters for a neural network model. You can refer to a specific layer using netId.parameters.layerId or to a specific element of a layer using netId.parameters.arrayId. For parameter arrays, individual indexing is not allowed.\nnetId.input: Input for a neural network with a single input argument (as in the first example here). If the input is provided as an array from a file, use netId.input. Otherwise, the input is assumed to be a Vector, where each element n should be mapped to a PEtab ID, with element n specified as netId.input[{n}].\nnetId.inputs: Inputs for a neural network with multiple input arguments (as in the second example here). Each input argument is accessed via netId.inputs[{n}], and for each argument the same rules apply as for netId.input. For example, to access element m in input n, write netId.inputs[{n}][{m}].\nnetId.output: Neural network output, assumed to be a Vector, where each element n should be mapped to a PEtab ID, with element n given by netId.input[{n}].","category":"page"},{"location":"format.html#Network-with-Scalar-Inputs","page":"Format","title":"Network with Scalar Inputs","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"For networks with scalar inputs, the PEtab entity should be a PEtab variable. For example, assume that the network net1 has two inputs, then a valid mapping table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_input1 net1.input[1]\nnet1_input2 net1.input[2]","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Scalar input variables can then be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Parameters in the parameters table: These may be either estimated or constant.\nParameters assigned in the condition or hybridization table: More details on this option can be found in the section on the condition and hybridization tables.","category":"page"},{"location":"format.html#Network-with-Array-Inputs","page":"Format","title":"Network with Array Inputs","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Sometimes, such as with image data, a neural network requires array input. In these situations, the input should be specified as an HDF5 file (for file structure here), which is given a suitable PEtab Id in the YAML file. This Id is then mapped to a PEtab variable in the mapping table. For example, given that net1 has a file input, a valid mapping table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_input_file net1.input","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Where net1_input_file has been specified in the problem YAML file.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"When multiple simulation conditions each require a different neural network array input, the mapping table should map the input to a PEtab variable (for example, net1_input):","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_input net1.input","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"This variable (here net1_input) is then assigned to specific input file variables (e.g. net1input_cond1 and net1_input_cond2) via the condition table using the setValue operator type. For a full example of a valid PEtab problem with array inputs, see [ADD].","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"TODO: Will fix an input format later.","category":"page"},{"location":"format.html#Network-with-Multiple-Input-Arguments","page":"Format","title":"Network with Multiple Input Arguments","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Sometimes a neural network model’s forward function has multiple input arguments. When the netId.input[{n}] notation is used in the mapping table it is assumed that there is only one input argument. Therefore, when there are multiple input arguments, the netId.inputs[{n}][{m}] notation should be used. For example, if there are two input arguments, each taking a scalar value, a valid mapping table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_arg1 net1.inputs[1][1]\nnet1_arg2 net1.inputs[2][1]","category":"page"},{"location":"format.html#output_obs","page":"Format","title":"Network Observable Formula Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"If the neural network output appears in the observable formula, the PEtab entity should be directly referenced in the observable formula. For example, given:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_output1 net1.output[1]","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"A valid entry in the observable table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"observableId observableFormula\nobs1 net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"As usual, the observableFormula can be any valid PEtab equation, so net1_output1 + 1 would also be valid.","category":"page"},{"location":"format.html#Network-Scalar-Output","page":"Format","title":"Network Scalar Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"If the output does not appear in the observable formula, the output variable should still be defined in the mapping table:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_output1 net1.output[1]","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"The output parameter (net1_output1) is then assigned in the condition or hybridization table (see below).","category":"page"},{"location":"format.html#mapping_ps","page":"Format","title":"Network Parameter Values","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The PEtab ID representing the parameters for a neural network model must also be assigned in the mapping table. For example, if the network is called net1, a valid mapping table entry would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_ps net1.parameters","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Next, net1_ps should be assigned properties in the parameter table.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It is also possible to target a specific layer in the parameter table. To do so, the layer must first be mapped to a PEtab variable. For the case above, a valid mapping table to target layer1 would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_ps net1.parameters\nnet1_ps_layer1 net1.parameters.layer1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It is also possible to target parameter arrays within a layer using the notation netId.layerId.arrayId.","category":"page"},{"location":"format.html#Additional-Details","page":"Format","title":"Additional Details","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Although a neural network can, in principle, accept both array and scalar inputs for a single argument, this feature is not currently tested for among tools implementing the PEtab SciML extension due to it being hard to implement. To have both scalar and array input, the neural network model should instead have multiple arguments for its forward pass function.","category":"page"},{"location":"format.html#hybrid_table","page":"Format","title":"Hybridization Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The PEtab SciML extension introduces a new hybridization table for assigning neural network inputs and outputs. Assignments made in this table apply to all conditions, and together with the condition table, it specifies where a neural network is inserted in a PEtab SciML problem. The hybridization table has the following columns:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"targetId operationType targetValue\nNONESTIMATEDENTITY_ID STRING MATH_EXPRESSION\nnet1_input1 setValue p1\nnet1_input2 setValue p1\n... ... ...","category":"page"},{"location":"format.html#Detailed-Field-Description","page":"Format","title":"Detailed Field Description","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"targetId [NON_ESTIMATED_ENTITY_ID, required]: The identifier of the non-estimated entity that will be modified. Restrictions vary depending on the operationType and the model type. Targets can be one of the following:\nDifferential Targets: Entities defined by a time derivative (e.g., targets of SBML rate rules or species that change by participating in reactions).\nAlgebraic Targets: Entities defined by an algebraic assignment (i.e., they are not associated with a time derivative and are generally not constant). In the context of a neural network, if the neural network appear in the observable formula or ODE RHS, its inputs are considered algebraic targets. If a neural network sets values prior to simulations, its inputs are considered to be a constant target (see below).\nConstant Targets: Entities defined by a constant value but may be subject to event assignments (e.g., SBML model parameters that are not targets of rate or assignment rules).\nModel Parameter Targets: Entities corresponding to model parameters in the model file (e.g., SBML model parameters).\noperationType [STRING, required]: Specifies the type of operation to be performed on the target. Allowed values are:\nsetValue: Sets the current value of the target to the value specified in targetValue. The target must be a constant target.\nsetRate: Sets the time derivative of the target to targetValue. The target must be a differential target.\nsetAssignment: Sets the target to the symbolic value of targetValue. The target must be an algebraic target.\nsetParameter: Assigns the value of a model parameter to a neural network output. In this case, targetValue must be a neural network output.\ntargetValue [STRING, required]: The value or expression that will be used to change the target. The interpretation of this value depends on the specified operationType.","category":"page"},{"location":"format.html#operationType-and-the-Condition-table-Defines-Hybrid-Model-Type","page":"Format","title":"operationType and the Condition table Defines Hybrid Model Type","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The operationType and condition table together specify where a neural network model is integrated into a PEtab SciML problem. In particular:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"ODE RHS: If all inputs use setAssignment and all outputs use either setRate or setParameter, the neural network appears in the ODE right-hand side.\nObservable Formula: If all inputs use setAssignment and no outputs are specified in the condition table, the neural network is part of the observable formula (note that the output variable must be referenced in the observable table).\nParameter/Initial Value Setting: If all inputs and outputs use setValue, or if they are only assigned in the condition table, the neural network sets model parameters or initial values before the simulation.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"All other combinations are disallowed because they generally do not make sense in a PEtab context. For example, if inputs use setAssignment and outputs use setValue, then parameter values prior to simulation would be set by an assignment rule derived from model equations. This is not allowed in PEtab because assignment rules might be time-dependent. Furthermore, if a parameter is to be assigned via a rule, it should already be incorporated into the model. The petab_sciml library provides a linter to ensure that no disallowed combination is used. Packages that do not wish to depend on Python are strongly encouraged to verify that the input combinations in the condition table are valid.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"note: Model structure altering assignments\nWhen setRate, setAssignment, or setParameter are used, the model structure or observable formula is altered during model import, effectively a neural network is inserted into the generated functions. Since such alterations apply to all conditions, these assignments can only be made in the hybridization table.","category":"page"},{"location":"format.html#Assigning-Neural-Network-Output","page":"Format","title":"Assigning Neural Network Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"To set a constant model parameter value before model simulations for all conditions, the setValue operator type should be used. For example, if parameter p is determined by net1_output1 (mapped to a neural network output in the mapping table) for all conditions, a valid hybridization table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"operationType targetId targetValue\nsetValue p net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Similar, to set an initial value, the setValue operator type should be used. For example, if the initial value of species X is determined by net1_output1 for all conditions, a valid hybridization table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"operationType targetId targetValue\nsetInitial X net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"These assignments above hold for all conditions. As explained here, condition-specific assignments are also possible when the neural network sets variables prior to model simulations.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"To set a model derivative, the setRate operator type should be used. For example, if the rate of species X is determined by net1_output1, a valid hybridization table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"operationType targetId targetValue\nsetRate X net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"To alter the ODE RHS, the setParameter operator type should be used.  For example, if an ODE model parameter p should be given by net1_output1, a valid hybridization table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"operationType targetId targetValue\nsetParameter p net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"In theory for this last example, the targetId (here p) can also appear in the observable formula. However, as described here, for the observable formula the mapped output variable must be directly encoded in the formula. Therefore, targetId appearing in the observable formula is not allowed, and the petab_sciml linter checks for this.","category":"page"},{"location":"format.html#Assigning-Neural-Network-Input","page":"Format","title":"Assigning Neural Network Input","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"When a neural network sets a constant model parameter value or initial value, its input variable (as specified in the mapping table) is treated as a standard PEtab variable. If that input variable is not defined in the parameter table, it should be assigned using the setValue operator type in the hybridization table if the assignment applies to all conditions; otherwise, it should be assigned in the condition table as explained here.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"When a neural network sets a model derivative or alters the ODE RHS, the input is considered to be an algebraic target. Therefore, the input variable should be assigned using the setAssignment operator. For example, if neural network input net1_input1 is given by specie X, a valid condition table is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"operationType targetId targetValue\nsetAssignment net_input1 X","category":"page"},{"location":"format.html#hybrid_cond_tables","page":"Format","title":"The Hybridization and the Condition table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"When the neural network sets model parameters and/or initial values prior to simulations, assignments for all conditions can be set with the setValue operator type (see above). However, sometimes assignments need to be condition-specific. In such cases, the neural network inputs and/or outputs should be assigned in the condition table rather than in the hybridization table. For example, in the output case, if net1_output1 sets p1 in cond1 and p2 in cond2, a valid condition table is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId targetId targetValue\ncond1 p1 net1_output1\ncond2 p2 net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Similarly, for the input case, if net1_input is given by p1 in cond1 and p2 in cond2, a valid condition table is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId targetId targetValue\ncond1 net1_input p1\ncond2 net1_input p2","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"note: Assignments for a network variables occur either in hybridization or condition table\nSince the hybridization table sets assignments for all conditions, variables assigned in the condition table cannot be assigned in the hybridization table, and vice versa. The linter will throw an error if a variable is assigned in both tables. If all assignments occur in the condition table, the hybridization table can be left empty or omitted.","category":"page"},{"location":"format.html#parameter_table","page":"Format","title":"Parameter Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The parameter table follows the same format as in PEtab version 2, with a subset of fields extended to accommodate neural network parameters and new initializationPriorType values for neural network-specific initialization. A general overview of the parameter table is available in the PEtab documentation; here, the focus is on extensions relevant to the SciML extension.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"note: Specific Assignments Have Precedence\nWhen parsing, more specific assignments (e.g., netId.layerId rather than netId) take precedence for nominal values, priors, and so on. This means that if netId has one prior, and netId.layerId has another, the more specific assignment netId.layerId overrides the less specific one netId for layerId in this case.","category":"page"},{"location":"format.html#Detailed-Field-Description-2","page":"Format","title":"Detailed Field Description","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId [String]: Identifies the neural network or a specific layer/parameter array. The target of the parameterId must be assigned via the mapping table.\nnominalValue [String \\| NUMERIC]: Specifies neural network nominal values. This can be:\nA PEtab variable that via the problem YAML file maps to an HDF5 file with the required structure. If no file exists at the given path when the problem is imported and the parameters are set to be estimated, a file is created with randomly sampled values.\nA numeric value applied to all parameters under parameterId.\nestimate [0 \\| 1]: Indicates whether the parameters are estimated (1) or fixed (0). This must be consistent across layers. For example, if netId has estimate = 0, then potential layer rows must also be 0. In other words, freezing individual network parameters is not allowed.\ninitializationPriorType [String, OPTIONAL]: Specifies the prior used for sampling initial values before parameter estimation. In addition to the PEtab-supported priors [ADD], the SciML extension supports the following standard neural network initialization priors:\nkaimingUniform (default) — with gain as initializationPriorParameters value.\nkaimingNormal — with gain as initializationPriorParameters value.\nxavierUniform — with gain as initializationPriorParameters value.\nxavierNormal — with gain as initializationPriorParameters value.","category":"page"},{"location":"format.html#Different-Priors-for-Different-Layers","page":"Format","title":"Different Priors for Different Layers","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Different layers can have distinct initialization prior parameters. For example, consider a neural-network model net1 where layer1 and layer2 require different initializationPriorParameters because they use different activation functions that need distinct gain values for the kaimingUniform prior. A valid parameter table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId parameterScale lowerBound upperBound estimate nominalValue initializationPriorType initializationPriorParameters\nnet1_ps lin -inf inf 1 net1_ps_file kaimingUniform 1\nnet1_layer1_ps lin -inf inf 1 net1_ps_file kaimingUniform 1\nnet1_layer2_ps lin -inf inf 1 net1_ps_file kaimingUniform 5/3","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Where parameterId are assumed to have been properly assigned in the mapping table. In this example, each layer references the same file variable for nominalValue. This means the layers obtain their values from the specified file. Unless a numeric value is provided for nominalValue, referring to the same file is required, since all neural network parameters should be collected in a single HDF5 file following the structure described here.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It is also possible to specify different priors for different parameter arrays within a layer. For example, to use different priors for the weights and bias in layer1 of net1 (assuming a linear layer), a valid parameter table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId parameterScale lowerBound upperBound estimate nominalValue initializationPriorType initializationPriorParameters\nnet1_ps lin -inf inf 1 net1_ps_file kaimingUniform 1\nnet1_layer1_weight lin -inf inf 1 net1_ps_file kaimingUniform 1\nnet1_layer1_bias lin -inf inf 1 net1_ps_file kaimingNormal 5/3","category":"page"},{"location":"format.html#Bounds-for-neural-net-parameters","page":"Format","title":"Bounds for neural net parameters","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Bounds can be specified for an entire network or its nested levels. However, it should be noted that most optimization algorithms used for neural networks, such as ADAM, do not support parameter bounds in their standard implementation.","category":"page"},{"location":"format.html#YAML_file","page":"Format","title":"Problem YAML File","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The PEtab problem YAML file follows the PEtab version 2 format, except that a mapping table is now required (it is optional in the standard). It also includes an extension section for specifying neural network YAML files, as well as any files for neural network parameters and/or inputs:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"extensions:\n  petab_sciml:\n    netId1:\n      location: file_path1.yaml\n    netId2:\n      location: file_path2.yaml\n    array_files:\n      netId1_input:\n        location: netId1_input.hdf5\n        language: hdf5\n      netId1_ps:\n        location: netId1_ps.hdf5\n        language: hdf5","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Here, netId1 and netId2 are the IDs of the neural network models, and net1_input and net1_ps corresponding to array files that can be used in the PEtab tables. In this case, net1_input would be used in the mapping table for netId1’s input, while netId1_ps would be the nominalValue in the parameter table.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"If the neural network is provided in another format—typically one specific to a certain implementation—the neural network library should be provided to inform users which library is used. For example, when using Lux.jl in Julia, a valid file would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"extensions:\n  petab_sciml:\n    netId1:\n      library: Lux.jl\n    netId2:\n      library: Lux.jl","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It is then up to the specific implementation to provide the neural network model during import of the PEtab problem.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Any number of neural networks can be specified. For example, for a model with a single neural network with ID net1, a valid YAML file would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"format_version: 2\nproblems:\n  - model_files:\n      model_sbml:\n        location: model.xml\n        language: sbml\n    measurement_files:\n      - measurements.tsv\n    observable_files:\n      - observables.tsv\n    condition_files:\n      - conditions.tsv\n    mapping_files:\n      - mapping_table.tsv\nparameter_file: parameters.tsv\nextensions:\n  petab_sciml:\n    net1:\n      location: net1.yaml\n    array_files:\n      net1_ps:\n        location: net1_ps.hdf5\n        language: hdf5","category":"page"},{"location":"index.html#PEtab-SciML-Extension","page":"Home","title":"PEtab SciML Extension","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The PEtab SciML extension expands the PEtab standard for parameter estimation problems to support hybrid models that combine data-driven neural network models with a mechanistic Ordinary Differential Equation (ODE) model. This enables a reproducible format for specifying and ultimately fitting hybrid models to time-series data. This repository contains both the format specification and a Python library for exporting neural network models to a standard YAML format, which can be imported across multiple programming languages.","category":"page"},{"location":"index.html#Major-Highlights","page":"Home","title":"Major Highlights","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"A file format that supports three approaches for combining mechanistic and neural network models:\nIncorporating neural network model(s) data-driven model in the ODE model right-hand side.\nIncorporating neural network model(s) in the observable formula which describes the mapping between simulation output and measurement data.\nIncorporating neural network model(s) to set constant model parameter values prior to simulation, allowing for example, available metadata to be used to set parameter values.\nSupport for many neural network architectures, including most standard layers and activation functions available in packages such as PyTorch.\nImplementations in tools across two programming languages. In particular, both PEtab.jl in Julia and AMICI in Python (Jax) can import problems in the PEtab SciML format.\nan extensive test suite that ensures the correctness of tools supporting the format.","category":"page"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Currently, an installation of git and a recent version of Python 3 is required. As usual, a Python virtual environment is encouraged.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"pip install git+https://github.com/sebapersson/petab_sciml#subdirectory=src/python&egg=petab_sciml","category":"page"},{"location":"index.html#Getting-help","page":"Home","title":"Getting help","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"If you have any problems with either using this package, or with creating a PEtab SciML problem, here are some helpful tips:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Please open an issue on GitHub.\nPost your questions in the #sciml-sysbio channel on the Julia Slack. While this is not a Julia package, the developers are active on that forum.","category":"page"},{"location":"tutorial.html#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"The tutorials will consist of:","category":"page"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"Overarching tutorial where we show how to implement the Lotka-Volterra problem from the UDE paper.\nExtended tutorial 1 where we have a neural-network setting parameters, here it is also worthwhile to consider simulation conditions.\nExtended tutorial 2 where we have a neural network in the observable function.","category":"page"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"If time, add tutorial for having two neural networks.","category":"page"}]
}
