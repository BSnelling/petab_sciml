var documenterSearchIndex = {"docs":
[{"location":"net_activation.html#layers_activation","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The PEtab SciML neural network model YAML format supports numerous standard neural network layers and activation functions. Layer names and associated keyword arguments follow the PyTorch naming scheme. PyTorch is used because it is currently the most popular machine learning framework, and its comprehensive documentation makes it easy to look up details for any specific layer or activation function.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"If support is lacking for a layer or activation function you would like to see, please file an issue on GitHub.","category":"page"},{"location":"net_activation.html#Supported-Neural-Network-Layers","page":"Supported Layers and Activation Functions","title":"Supported Neural Network Layers","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The table below lists the supported and tested neural network layers along with links to their respective PyTorch documentation. Additionally, the table indicates which tools support each layer.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"layer PEtab.jl AMICI\nLinear x \nBilinear x \nFlatten x \nDropout x \nDropout1d x \nDropout2d x \nDropout3d x \nAlphaDropout x \nConv1d x \nConv2d x \nConv3d x \nConvTranspose1d x \nConvTranspose2d x \nConvTranspose3d x \nMaxPool1d x \nMaxPool2d x \nMaxPool3d x \nAvgPool1d x \nAvgPool2d x \nAvgPool3d x \nLPPool1 x \nLPPool2 x \nLPPool3 x \nAdaptiveMaxPool1d x \nAdaptiveMaxPool2d x \nAdaptiveMaxPool3d x \nAdaptiveAvgPool1d x \nAdaptiveAvgPool2d x \nAdaptiveAvgPool3d x ","category":"page"},{"location":"net_activation.html#Supported-Activation-Function","page":"Supported Layers and Activation Functions","title":"Supported Activation Function","text":"","category":"section"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"The table below lists the supported and tested activation functions along with links to their respective PyTorch documentation. Additionally, the table indicates which tools support each layer.","category":"page"},{"location":"net_activation.html","page":"Supported Layers and Activation Functions","title":"Supported Layers and Activation Functions","text":"Function PEtab.jl AMICI\nrelu x \nrelu6 x \nhardtanh x \nhardswish x \nselu x \nleaky_relu x \ngelu x \ntanhshrink x \nsoftsign x \nsoftplus x \ntanh x \nsigmoid x \nhardsigmoid x \nmish x \nelu x \ncelu x \nsoftmax x \nlog_softmax x ","category":"page"},{"location":"format.html#Format-Specification","page":"Format","title":"Format Specification","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"A PEtab SciML problem extends the PEtab standard version 2 to accommodate hybrid models SciML problems that combine data-driven (neural net) and mechanistic components. The extension introduces one new PEtab file type:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Neural Net File(s): YAML file(s) describing neural net model(s).","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"It further extends the following standard PEtab files:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Mapping Table: Used to describe how neural network inputs and outputs map to PEtab quantities.\nParameters Table: Used to describe nominal values and potential priors for initializing network parameters.\nCondition Table: Used to assign neural network outputs and inputs.\nProblem YAML File: Includes a new SciML field.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"All other PEtab files remain unchanged. This page explains the format and options for each file that is added or modified by the PEtab SciML extension.","category":"page"},{"location":"format.html#High-Level-Overview","page":"Format","title":"High Level Overview","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The main goal of the PEtab SciML extension is to enable hybrid models that combine data-driven and mechanistic components. There are three types of hybrid model considered, each specified differently:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Data-driven models in the ODE model’s right-hand side (RHS): In this scenario, the SBML file is modified during import by either replacing a derivative or assigning a parameter to a neural network output. In both cases, the neural network input and output variables (as defined in the mapping table) must be assigned in the condition table using the setNetRate and/or setNetAssignment operator types.\nData-driven models in the observable function: In this scenario, the neural network output variable (as defined in the mapping table) is directly embedded in the observable formula. Meanwhile, the input variables (also defined in the mapping table) are assigned in the condition table using the setNetAssignment operator type.\nData-driven models before the ODE model: In this scenario, the data-driven model sets constant parameters or initial values in the ODE model prior to simulation. The input variable (as defined in the mapping table) can be assigned in the parameter or condition table as a standard constant PEtab variable, and the output variables (as defined in the mapping table) are assigned via the condition table.","category":"page"},{"location":"format.html#Neural-Network-Model-Format","page":"Format","title":"Neural Network Model Format","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"TODO: Dilan, I will need some help from you here, link the scheme?","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Neural network models are provided as separate YAML files, and each tool supporting the extension is responsible for importing this file into a suitable format. A neural network YAML file has two main sections:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"layers: Defines the neural network layers, each with a unique ID. The layer names and argument syntax follow PyTorch conventions.\nforward: Describes the forward pass, specifying the order of layer calls and any applied activation functions.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Although network YAML files can be manually written, the recommended approach is to define a PyTorch nn.Module whose constructor sets up the layers and whose forward method specifies how they are invoked. For example, a simple feed-forward network can be defined as:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Linear(in_features=2, out_features=5)\n        self.layer2 = nn.Linear(in_features=5, out_features=5)\n        self.layer3 = nn.Linear(in_features=5, out_features=1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = F.tanh(x)\n        x = self.layer2(x)\n        x = F.tanh(x)\n        x = self.layer3(x)\n        return x","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"The corresponding YAML file can then be generated using the petab_sciml library:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"# TODO: Add","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Any PyTorch-supported keyword can be supplied for each layer in the YAML file, allowing for a broad range of architectures. For example, a more complex convolutional model might be structured as:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.max_pool1 = nn.MaxPool2d((2, 2))\n        self.fc1 = nn.Linear(64, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.flatten1 = nn.Flatten()\n\n    def forward(self, input):\n        c1 = self.conv1(input)\n        s2 = self.max_pool1(c1)\n        c3 = self.conv2(s2)\n        s4 = self.max_pool1(c3)\n        s4 = self.flatten1(s4)\n        f5 = self.fc1(s4)\n        f6 = self.fc2(f5)\n        output = self.fc3(f6)\n        return output","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"A complete list of supported and tested layers and activation functions can be found here.","category":"page"},{"location":"format.html#Neural-Network-Parameters","page":"Format","title":"Neural Network Parameters","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"All parameters for a neural network model are stored in an HDF5 file, with the file name specified in the parameter table. In this file, each layer’s parameters are grouped under f.layerId, where layerId is the layer’s unique identifier. For example, the weights of a linear layer are stored at f.layerId.weight.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Since parameters are stored in an HDF5 format, they are stored as arrays. The indexing follows PyTorch conventions, meaning parameters are stored in row-major order. Typically, users do not need to manage these details manually, as PEtab SciML tools should handle them automatically.","category":"page"},{"location":"format.html#Neural-Network-Input","page":"Format","title":"Neural Network Input","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"When network input is provided as an array via an HDF5 file (see the mapping table below), it should follow the PyTorch convention. For example, if the first layer is Conv2d, the input should be in (C, W, H) format, with data stored in row-major order. In general, the input should be structured to be directly compatible with PyTorch.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"tip: For developers: Respect memory order\nTools supporting the SciML extension should, for computational efficiency, reorder input data and potential layer parameter arrays to match the memory ordering of the target language. For example, PEtab.jl converts input data to column-major order, as used in Julia.","category":"page"},{"location":"format.html#Mapping-Table","page":"Format","title":"Mapping Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The mapping table describes whcih PEtab problem variables a neural network’s inputs and outputs map. Each neural network input and output must be mapped in the mapping table.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\ne.g. \nk1 netId.input1","category":"page"},{"location":"format.html#Detailed-Field-Descriptions","page":"Format","title":"Detailed Field Descriptions","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId [STRING]: A valid PEtab identifier not defined elsewhere in the PEtab problem. It can be referenced in the condition, measurement, parameter, and observable tables or be a file, but not in the model itself. For neural network outputs, the PEtab identifier must be assigned in the condition table, whereas for inputs, this is not required (see examples below).\nmodelEntityId [STRING]: Describes the neural network entity corresponding to the petabEntityId. Must follow the format netId.input{n} or netId.output{n}, where n is the specific input or output index.","category":"page"},{"location":"format.html#Network-with-Scalar-Inputs","page":"Format","title":"Network with Scalar Inputs","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"For networks with scalar inputs, the PEtab entity should be a PEtab variable. For example, assume that the network net1 has two inputs, then a valid mapping table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_input1 net1.input1\nnet1_input2 net1.input2","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Scalar input variables can then be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Parameters in the parameters table: These may be either estimated or constant.\nParameters assigned in the condition table: More details on this option can be found in the section on the condition table.","category":"page"},{"location":"format.html#Network-with-Array-Inputs","page":"Format","title":"Network with Array Inputs","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Sometimes, such as with image data, a neural network requires array input. In these cases, the input can be specified as an HDF5 file path directly in the mapping table:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\ninput_net1.hdf5 net1.input1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"As mentioned in [ADD], the HDF5 file should follow PyTorch indexing and be stored in row-major order.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"When there are multiple simulation conditions that each require a different neural network array input, the mapping table should map to a PEtab variable (e.g., net1_input):","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_input net1.input1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"This variable is then assigned to specific input files via the condition table using the setValue operatorType. For a full example of a valid PEtab problem with array inputs, see [ADD].","category":"page"},{"location":"format.html#Network-Observable-Formula-Output","page":"Format","title":"Network Observable Formula Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"If the neural network output appears in the observable formula, the PEtab entity should be directly referenced in the observable formula. For example, given:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_output1 net1.output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"A valid observable table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"observableId observableFormula\nobs1 net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"As usual, the observableFormula can be any valid PEtab equation, so net1_output1 + 1 would also be valid.","category":"page"},{"location":"format.html#Network-Scalar-Output","page":"Format","title":"Network Scalar Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"If the output does not appear in the observable, the output variable should still be defined in the mapping table:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"petabEntityId modelEntityId\nnet1_output1 net1.output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"The output parameter (net1_output1) is then assigned in the condition table (see below).","category":"page"},{"location":"format.html#Additional-Details","page":"Format","title":"Additional Details","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Although a neural network can, in principle, accept both array and scalar inputs, this feature is not currently tested for among tools implementing the PEtab SciML extension due to it being hard to implement. However, tools are free to add this feature.","category":"page"},{"location":"format.html#Condition-Table","page":"Format","title":"Condition Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"In the PEtab SciML extension, the condition table is extended to specify how neural network outputs (and, if necessary, inputs) are assigned. Two new operatorType are introduced in the extension to support this functionality:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"setNetRate: Assigns the rate of a species to a neural network output. Here, targetValue must be a neural network output and targetId must be a model specie.\nsetNetAssignment: Assigns the input or output of a neural network in the ODE right-hand side (RHS) or the input in the observable formula.\nInput Case: targetId is a neural network input, and targetValue can be any valid PEtab math expression that references model variables.\nOutput Case: targetId is a non-estimated ODE model parameter, and targetValue is a neural network output. This is used to assign a neural network output in the ODE model RHS.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"warn: Model structure altering conditions\nIWhen setNetRate or setNetAssignment are used during model import the generated model structure or observable formula is altered, basically a neural-network is inserted into the generated functions. Therefore, as unique model structures per condition are not supported in most PEtab tools, the same setNetAssignment or setNetRate assignment must be set per condition.","category":"page"},{"location":"format.html#Assigning-Neural-Network-Output","page":"Format","title":"Assigning Neural Network Output","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"To set a constant model parameter value before model simulations, the setValue operator type should be used. For example, if parameter p is determined by net1_output1 (mapped to a neural network output in the mapping table), a valid condition table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId operatorType targetId targetValue\ncond1 setValue p net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Note that this specification allows for condition-specific assignments. For example, net1_output1 could target a different parameter in another condition or multiple model parameters in the same condition.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"To set an initial value, the setInitial operator type should be used. For example, if the initial value of species X comes from net1_output1, a valid condition table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId operatorType targetId targetValue\ncond1 setInitial X net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"To set a model derivative, the setNetRate operator type should be used. For example, if the rate of species X is given by net1_output1, a valid condition table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId operatorType targetId targetValue\ncond1 setNetRate X net1_output1","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"To alter the ODE RHS, the setNetAssignment operator type should be used. For example, if an ODE model parameter p should be given by net1_output1, a valid condition table entry is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId operatorType targetId targetValue\ncond1 setNetAssignment p net1_output1","category":"page"},{"location":"format.html#Assigning-Neural-Network-Input","page":"Format","title":"Assigning Neural Network Input","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"When a neural network sets a constant model parameter value or initial value, its input variable (as specified in the mapping table) is a standard PEtab variable. If that input variable is not defined in the parameter table, it should be assigned using the setValue operator type.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"When a neural network sets a model derivative or alters the ODE RHS, the input typically depends on model entities. Therefore, the input variable should be assigned using the setNetAssignment operator. For example, if neural network input net1_input1 is given by specie X, a valid condition table is:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"conditionId operatorType targetId targetValue\ncond1 setNetAssignment net_input1 X","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Note, to ensure correct mapping, setNetAssignment must always be used for inputs when the neural network is part of the ODE RHS or observable formula.","category":"page"},{"location":"format.html#operatorType-Defines-Hybrid-Model-Type","page":"Format","title":"operatorType Defines Hybrid Model Type","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The condition table and mapping table together specify where a neural network model is located in a PEtab SciML problem. In particular:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"If all inputs use setNetAssignment and all outputs use either setNetRate or setNetAssignment, the neural network appears in the ODE RHS.\nIf all inputs use setNetAssignment and no outputs appear in the condition table, the neural network is part of the observable formula (note that the output variable must be referenced in the observable table).\nIf no inputs use setNetAssignment and all outputs use either setValue or setInitial, the neural network sets model parameters or initial values before the simulation.","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"All other combinations are disallowed because they generally do not make sense in a PEtab context. For example, if inputs use setNetAssignment and outputs use setValue, parameter values prior to simulation would be set via an assignment rule consisting of model equations, which is not permitted in PEtab as assignment rules might be time-dependent. Moreover, if a parameter is to be set via an assignment rule, this should already be coded in the model. Implementations must ensure that the input combinations in the condition table are valid.","category":"page"},{"location":"format.html#Parameter-Table","page":"Format","title":"Parameter Table","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The parameter table follows the same format as in PEtab version 2, with a subset of fields extended to accommodate neural network parameters and new initializationPriorType values for neural network-specific initialization. A general overview of the parameter table is available in the PEtab documentation; here, the focus is on extensions relevant to the SciML extension.","category":"page"},{"location":"format.html#Detailed-Field-Description","page":"Format","title":"Detailed Field Description","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId [String]: Identifies the neural network or a specific layer/parameter array. For example, layerId for netId can be specified using netId.layerId. A row for netId must be defined in the table. When parsing, more specific levels (e.g., netId.layerId) take precedence for nominal values, priors, etc.\nnominalValue [String \\| NUMERIC]: Specifies neural network nominal values. This can be:\nA path to an HDF5 file that follows PyTorch syntax (recommended, see above for file format). If no file exists when the problem is imported and the parameters are set to be estimated, a file is created with randomly sampled values.\nA numeric value applied to all parameters under netId.\nestimate [0 \\| 1]: Indicates whether the parameters are estimated (1) or fixed (0). This must be consistent across layers. For example, if netId has estimate = 0, then netId.layerId must also be 0. In other words, freezing individual network parameters is not allowed.\ninitializationPriorType [String, OPTIONAL]: Specifies the prior used for sampling initial values before parameter estimation. In addition to the PEtab-supported priors [ADD], the SciML extension supports the following standard neural network initialization priors:\nkaimingUniform (default) — with gain as initializationPriorParameters value.\nkaimingNormal — with gain as initializationPriorParameters value.\nxavierUniform — with gain as initializationPriorParameters value.\nxavierNormal — with gain as initializationPriorParameters value.","category":"page"},{"location":"format.html#Different-Priors-for-Different-Layers","page":"Format","title":"Different Priors for Different Layers","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Different layers can be defined for different layers. For example, consider a neural-network model net1 where layer1 and layer2 should have different initializationPriorParameters, because they use different activation functions that should distinct gain for the kaimingUniform prior. A valid parameter table:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId parameterScale lowerBound upperBound estimate nominalValue initializationPriorType initializationPriorParameters\nnet1 lin -inf inf 1 net1_ps.hf5 kaimingUniform 1\nnet1.layer1 lin -inf inf 1 net1_ps.hf5 kaimingUniform 1\nnet1.layer2 lin -inf inf 1 net1_ps.hf5 kaimingUniform 5/3","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"If is also possible to have different priors for different parameter arrays in a layer. For example, to use different priors for the weights and bias in layer1 of net1, which is, for instance, a linear layer. In this case, a valid parameter table would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"parameterId parameterScale lowerBound upperBound estimate nominalValue initializationPriorType initializationPriorParameters\nnet1 lin -inf inf 1 net1_ps.hf5 kaimingUniform 1\nnet1.layer1.weight lin -inf inf 1 net1_ps.hf5 kaimingUniform 1\nnet1.layer1.bias lin -inf inf 1 net1_ps.hf5 kaimingNormal 5/3","category":"page"},{"location":"format.html#Bounds-for-neural-net-parameters","page":"Format","title":"Bounds for neural net parameters","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"Bounds can be specified for an entire network or its nested levels. However, it should be noted that most optimization algorithms used for neural networks, such as ADAM, do not support parameter bounds in their standard implementation.","category":"page"},{"location":"format.html#Problem-YAML-File","page":"Format","title":"Problem YAML File","text":"","category":"section"},{"location":"format.html","page":"Format","title":"Format","text":"The PEtab problem YAML file follows the format of PEtab version 2, except that a mapping table is required (it is optional in the standard). It also includes an extension section to specify the neural network YAML files:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"extensions:\n  petab_sciml:\n    netId1:\n      file: \"file_path1.yaml\"\n    netId2:\n      file: \"file_path2.yaml\"","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"Here, netId1 and netId2 are the IDs of the neural network models. Note that any number of neural networks can be specified. For example, for a model with one neural network, with ID net1, a valid YAML file would be:","category":"page"},{"location":"format.html","page":"Format","title":"Format","text":"format_version: 2\nproblems:\n  - model_files:\n      model_sbml:\n        location: \"model.xml\"\n        language: \"sbml\"\n    measurement_files:\n      - \"measurements.tsv\"\n    observable_files:\n      - \"observables.tsv\"\n    condition_files:\n      - \"conditions.tsv\"\n    mapping_files:\n      - \"mapping_table.tsv\"\nparameter_file: \"parameters.tsv\"\nextensions:\n  petab_sciml:\n    net1:\n      file: \"net1.yaml\"","category":"page"},{"location":"index.html#PEtab-SciML-Extension","page":"Home","title":"PEtab SciML Extension","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The PEtab SciML extension expands the PEtab standard for parameter estimation problems to support hybrid models that combine data-driven neural network models with a mechanistic Ordinary Differential Equation (ODE) model. This enables a reproducible format for specifying and ultimately fitting hybrid models to time-lapse data. This repository contains both the format specification and a Python library for exporting neural network models to a standard YAML format, which can be imported across multiple programming languages.","category":"page"},{"location":"index.html#Major-Highlights","page":"Home","title":"Major Highlights","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"A format which supports three approaches for combining mechanistic and neural network models:\nIncorporating neural network model(s) data-driven model in the ODE model right-hand side.\nIncorporating neural network model(s) in the observable formula which describes the mapping between simulation output and measurement data.\nIncorporating neural network model(s) to set constant model parameter values prior to simulation, allowing for example, available metadata to be used to set parameter values.\nFormat which supports many neural network architectures, including most standard layers and activation functions available in packages such as PyTorch.\nFormat supported in tools across several programming languages. In particular, both PEtab.jl in Julia and AMICI in Python (Jax) can import problems in the PEtab SciML format.\nAn extensive test suite that ensures the correctness of tools supporting the format.","category":"page"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"TODO: Dilan please help here. Will need to setup with pip?","category":"page"},{"location":"index.html#Getting-help","page":"Home","title":"Getting help","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"If you have any problems with either using this package, or with creating a PEtab SciML problem, here are some helpful tips:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Please open an issue on GitHub.\nPost your questions in the #sciml-sysbio channel on the Julia Slack. While this is not a Julia package, the developers are active on that forum.","category":"page"},{"location":"tutorial.html#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"The tutorials will consist of:","category":"page"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"Overarching tutorial where we show how to Lotka-Voltera problem from the UDE paper.\nExtended tutorial 1 where we have a neural-network setting parameters, here it is also worthwhile to consider simulation conditions.\nExtended tutorial 2 where we have a neural network in the observable function.","category":"page"},{"location":"tutorial.html","page":"Tutorial","title":"Tutorial","text":"If time, add tutorial for having two neural networks.","category":"page"}]
}
